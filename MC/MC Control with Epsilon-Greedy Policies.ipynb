{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.blackjack import BlackjackEnv\n",
    "from lib import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        \n",
    "        # Implement this!\n",
    "\n",
    "        best_action = np.argmax(Q[observation])\n",
    "\n",
    "        response_probability = (epsilon/nA)*np.ones(nA)\n",
    "        response_probability[best_action] += 1-epsilon\n",
    "\n",
    "\n",
    "        \n",
    "        return response_probability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unit test for make_epsilon_greedy_policy:\n",
    "\n",
    "\n",
    "# for \n",
    "\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 2., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = defaultdict(lambda: np.zeros(10))\n",
    "\n",
    "Q['9'][7] +=1\n",
    "Q['9'][7] +=1\n",
    "Q['9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(np.array([0, 1]), (1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [5., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = defaultdict(lambda: np.zeros([10, 2]))\n",
    "\n",
    "Q['0'][4] = np.array([5, 1])\n",
    "\n",
    "Q['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning points in the following exercise:\n",
    "\n",
    "### To outline different ways to go through an episode, where the signal to stop is when the environment returns 'done' as 'true'\n",
    "\n",
    "### Note that if there is a possibility that the epsiode can go on without reaching an end, i.e. when the environment can be trapped in a loop, the algorithm might never end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) # all action-values are started to be zero\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    # Implement this!\n",
    "\n",
    "    ## loop for the specified number of episodes:\n",
    "    for n in num_episodes:\n",
    "\n",
    "        \n",
    "        # 2. generate an episode using the policy (to be iterated) so far:\n",
    "        \n",
    "        # start by reseting the env:\n",
    "        observation = env.reset()\n",
    "\n",
    "        state_action_return_and_factor = defaultdict(lambda: np.tiles(np.array([0, 1]), (env.action_space, 1) ) )\n",
    "        while True:\n",
    "            # select an action according to the (possibly stochastic) policy\n",
    "            action = np.random.multinomial(1, policy(observation))\n",
    "\n",
    "            # the environment takes the action and give a new state (observation)\n",
    "            # , reward, and \n",
    "            new_observation, reward, done, _ = env.step(action)\n",
    "\n",
    "            # start adding the rewards for each state-action pairs\n",
    "            # we need also to record the accum discount fact for that pairs:\n",
    "\n",
    "            \n",
    "            accum_reward, accum_discount = state_action_return_and_factor[observation][action]\n",
    "            new_accum_discount = discount_factor*accum_discount\n",
    "            state_action_return_and_factor[observation][action] = np.array( [accum_reward+ new_accum_discount*reward, new_accum_discount])\n",
    "\n",
    "\n",
    "\n",
    "        # 3.after finishing one episode:\n",
    "        # 3.1 update the \n",
    "        # \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=500000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For plotting: Create value function from action-value function\n",
    "# by picking the best action at each state\n",
    "V = defaultdict(float)\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value\n",
    "plotting.plot_value_function(V, title=\"Optimal Value Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
