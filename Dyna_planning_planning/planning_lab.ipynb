{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing the performance of different algorithms on :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.envs.cliff_walking import CliffWalkingEnv\n",
    "from Dyna_planning_planning.tabular_dyna_q import random_sample_one_step_Q_planning, tabular_Dyna_Q\n",
    "\n",
    "\n",
    "clif_env = CliffWalkingEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional or an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:941\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m clif_env \u001b[38;5;241m=\u001b[39m CliffWalkingEnv()\n\u001b[0;32m----> 2\u001b[0m Q_dyna, stats \u001b[38;5;241m=\u001b[39m \u001b[43mtabular_Dyna_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclif_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_real_episode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_loop_in_between_real\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/reinforcement_learning_models/learning_gynasium_with_textbook_exercises/reinforcement-learning/Dyna_planning_planning/../Dyna_planning_planning/tabular_dyna_q.py:187\u001b[0m, in \u001b[0;36mtabular_Dyna_Q\u001b[0;34m(env, num_real_episode, num_loop_in_between_real, alpha, discount_factor, epsilon)\u001b[0m\n\u001b[1;32m    184\u001b[0m stats\u001b[38;5;241m.\u001b[39mepisode_lengths[epi] \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# 1-step TD update using real experience:\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m td_target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m discount_factor\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax(Q[next_state])\n\u001b[1;32m    188\u001b[0m td_delta \u001b[38;5;241m=\u001b[39m td_target \u001b[38;5;241m-\u001b[39m Q[state][action]\n\u001b[1;32m    189\u001b[0m Q[state][action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m td_delta\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:943\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional or an integer"
     ]
    }
   ],
   "source": [
    "clif_env = CliffWalkingEnv()\n",
    "Q_dyna, stats = tabular_Dyna_Q(clif_env, num_real_episode = 500, num_loop_in_between_real = 50, alpha = 0.5, discount_factor = 1.0, epsilon = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_with_gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
